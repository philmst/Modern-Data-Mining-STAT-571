---
title: "COVID-19 Case Study: Group 25"
author:
- "Brandon Kleinman"
- "Philip Situmorang"
- "Ben Sra Chongbanyatcharoen"
date: 'Due before midnight, Feb 27'
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
    number_sections: yes
  pdf_document:
    toc_depth: '4'
    number_sections: yes
urlcolor: blue
---

```{r Setup, include=FALSE, results='hide', warning=FALSE}
knitr::opts_chunk$set(echo = T, fig.width=8, fig.height=4)
options(scipen = 0, digits = 3)  # controls base R output

# Package setup
if(!require("pacman")) install.packages("pacman")

pacman::p_load(tidyverse, dplyr, ggplot2, ggthemes, data.table, lubridate, leaps,
               GGally, RColorBrewer, ggsci, plotROC, usmap, gridExtra, sandwich,
               plotly, ggpubr, vistime, glmnet, mltools, car, stargazer, sf)

output_format <- ifelse(is.null(knitr::opts_knit$get("rmarkdown.pandoc.to")),
                        "text", knitr::opts_knit$get("rmarkdown.pandoc.to"))

options(dplyr.summarise.inform = FALSE)
```

# Background

The outbreak of the novel Corona virus disease 2019 (COVID-19) [was declared a public health emergency of international concern by the World Health Organization (WHO) on January 30, 2020](https://www.who.int/dg/speeches/detail/who-director-general-s-statement-on-ihr-emergency-committee-on-novel-coronavirus-(2019-ncov)). Upwards of [112 million cases have been confirmed worldwide, with nearly 2.5 million associated deaths](https://covid19.who.int/). Within the US alone, there have been [over 500,000 deaths and upwards of 28 million cases reported](https://covid.cdc.gov/covid-data-tracker/#trends_dailytrendscases). Governments around the world have implemented and suggested a number of policies to lessen the spread of the pandemic, including mask-wearing requirements, travel restrictions, business and school closures, and even stay-at-home orders. The global pandemic has impacted the lives of individuals in countless ways, and though many countries have begun vaccinating individuals, the long-term impact of the virus remains unclear.

The impact of COVID-19 on a given segment of the population appears to vary drastically based on the socioeconomic characteristics of the segment. In particular, differing rates of infection and fatalities have been reported among different [racial groups](https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-race-ethnicity.html), [age groups](https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-age.html), and [socioeconomic groups](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7221360/). One of the most important metrics for determining the impact of the pandemic is the death rate, which is the proportion of people within the total population that die due to the the disease. 

We assemble this dataset for our research with the goal to investigate the effectiveness of lockdown on flattening the COVID curve. We provide a portion of the cleaned dataset for this case study. 

There are two main goals for this case study. 

1. We show the dynamic evolvement of COVID cases and COVID-related death at state level.
2. We try to figure out what county-level demographic and policy interventions are associated with mortality rate in the US. We try to construct models to find possible factors related to county-level COVID-19 mortality rates.
3. This is a rather complex project. With our team's help we have made your job easier. 
4. Hide all unnecessary lengthy R-output. Keep your write up neat, readable. 

  
**Remark1:** The data and the statistics reported here were collected before February of 2021. 

**Remark 2:** A group of RAs spent tremendous amount of time working together to assemble the data. It requires data wrangling skills. 

**Remark 3:** Please keep track with the most updated version of this write-up.


# Data Summary

The data comes from several different sources: 

1. [County-level infection and fatality data](https://github.com/nytimes/covid-19-data) - This dataset gives daily cumulative numbers on infection and fatality for each county. 
    * [NYC data](https://github.com/nychealth/coronavirus-data)
2. [County-level socioeconomic data](https://www.ers.usda.gov/data-products/atlas-of-rural-and-small-town-america/download-the-data/) - The following are the four relevant datasets from this site.   
    i. Income - Poverty level and household income. 
    ii. Jobs - Employment type, rate, and change.
    iii. People - Population size, density, education level, race, age, household size, and migration rates.
    iv. County Classifications - Type of county (rural or urban on a rural-urban continuum scale).
3. [Intervention Policy Data](https://github.com/JieYingWu/COVID-19_US_County-level_Summaries/blob/master/data/interventions.csv) - This dataset is a manually compiled list of the dates that interventions/lockdown policies were implemented and lifted at the county level. 

# EDA

In this case study, we use the following three nearly cleaned data:

* **covid_county.csv**: County-level socialeconomic information that combines the above-mentioned 4 datasets: Income (Poverty level and household income), Jobs (Employment type, rate, and change), People (Population size, density, education level, race, age, household size, and migration rates), County Classifications
* **covid_rates.csv**: Daily cumulative numbers on infection and fatality for each county
* **covid_intervention.csv**: County-level lockdown intervention.

Among all data, the unique identifier of county is `FIPS`.

The cleaning procedure is attached in `Appendix 2: Data cleaning` You may go through it if you are interested or would like to make any changes. 

**It may need more data wrangling.**

First read in the data.

```{r}
# county-level socialeconomic information
county_data <- fread("data/covid_county.csv") 
# county-level COVID case and death
covid_rate <- fread("data/covid_rates.csv")
# county-level lockdown dates 
# covid_intervention <- fread("data/covid_intervention.csv")
```

## Understand the data

The detailed description of variables is in `Appendix 1: Data description`. Please get familiar with the variables. Summarize the two data briefly.
```{r, eval=FALSE}
dim(county_data)
names(county_data)
str(county_data)
```
**ANSWER:** county_data breaks down various demographic and socioeconomic variables by U.S. county. Each county is uniquely identified by FIPS variable, which identifies the U.S state or territory and county of a given county. States are indicated by the first one or two digits (1 for AL, 2 for AK, ..., 72 for PR) followed by numbers which indicate the county within that state.

Covid_rate breaks down the daily number of COVID cases and deaths of each county (uniquely identified by FIPS). The observations span from January 2020 to February 2021.

## COVID case trend

It is crucial to decide the right granularity for visualization and analysis. We will compare daily vs weekly total new cases by state and we will see it is hard to interpret daily report.

i) Plot **new** COVID cases in NY, WA and FL by state and by day. Any irregular pattern? What is the biggest problem of using single day data? 

**ANSWER: ** The biggest problem is the volatility in number of new cases per day. Moreover, also saw that the number of new cases in FL is zero for a number of days. This may be due to untimely data reporting that led to some cases being recorded after the day it was detected. It can be difficult to analyze trend when the y variable rises and falls frequently. 

```{r, echo=FALSE}
num_rows <- nrow(covid_rate)
cum_cases <- covid_rate$cum_cases
FIPS <- covid_rate$FIPS
new_cases <-c(rep(NA,num_rows))

for(i in 2:num_rows){
  new_case <- cum_cases[i] - cum_cases[i-1]
  if (FIPS[i] == FIPS[i-1]) {
    new_cases[i]<- new_case
  } else {
    new_cases[i]<- cum_cases[i]
  }
}

covid_rate$new_cases <- new_cases
```

```{r, include = FALSE}
covid_rate_NY <- covid_rate %>% filter(State == "New York") %>% group_by(date) %>% summarize(new_cases = sum(new_cases))

covid_rate_NY$Day <-  seq(1, nrow(covid_rate_NY))
```

```{r, include = FALSE}
covid_rate_WA <- covid_rate %>% filter(State == "Washington") %>% group_by(date) %>% summarize(new_cases = sum(new_cases))

covid_rate_WA$Day <-  seq(1, nrow(covid_rate_WA))
```

```{r, include = FALSE}
covid_rate_FL <- covid_rate %>% filter(State == "Florida") %>% group_by(date) %>% summarize(new_cases = sum(new_cases))

covid_rate_FL$Day <-  seq(1, nrow(covid_rate_FL))

unique(rownames(covid_rate_FL)[new_cases==0])
```

<br>
Here are the plots of daily new COVID cases in NY, WA, and FL.
<br>

```{r, echo=FALSE}
ggplot(data=covid_rate_NY, aes(date, new_cases)) +
theme_minimal() +
geom_line(color = "#00AFBB", size = .4) +
labs(title = "NY Daily New Cases") 
```

```{r, echo=FALSE}
ggplot(data=covid_rate_WA, aes(date, new_cases)) +
theme_minimal() +
geom_line(color = "#FC4E07", size = .4) +
labs(title = "WA Daily New Cases") 
```

```{r, echo=FALSE}
ggplot(data=covid_rate_FL, aes(date, new_cases)) +
theme_minimal() +
geom_line(color = "darkmagenta", size = .4) +
labs(title = "FL Daily New Cases") 
```

ii) Create **weekly new** cases per 100k `weekly_case_per100k`. Plot the spaghetti plots of `weekly_case_per100k` by state. Use `TotalPopEst2019` as population.

```{r}
#calculate the population of each state
state_pop <- covid_rate %>% 
  group_by(State, County) %>% 
  summarize(county_pop = mean(TotalPopEst2019)) %>% 
  group_by(State) %>% 
  summarize(state_pop = sum(county_pop))

#calculate weekly new cases
covid_rate_week <- covid_rate %>% 
  group_by(State, week) %>% 
  summarize(weekly_new_cases = sum(new_cases)) 

#merge the population data with weekly new cases data
covid_rate_week <- 
  merge(x=covid_rate_week,y=state_pop,by="State",all.x=TRUE)

#calculate weekly_new_case_per100k
covid_rate_week <- 
  covid_rate_week %>% 
  mutate(weekly_case_per100k = (weekly_new_cases/(state_pop) * 100000))
```

iii) Summarize the COVID case trend among states based on the plot in ii). What could be the possible reasons to explain the variabilities?

**ANSWER: ** 
We can see that the each state experiences a surge ("wave") in new covid cases of differing degrees, and at different times.
There are a number of possible reasons for this difference between the three states such as:

- the population density (i.e. how likely is it for people be in close proximity of each other)
- public health policies enforced (i.e. masking regulation, lockdown)

```{r, include=FALSE}
covid_wk_rate_NY <- covid_rate_week %>% filter(State == "New York")
covid_wk_rate_WA <- covid_rate_week %>% filter(State == "Washington")
covid_wk_rate_FL <- covid_rate_week %>% filter(State == "Florida")
```

Below are the plots on new weekly covid cases per 100k population for each state.
```{r, echo=FALSE}
ggplot(data=covid_wk_rate_NY, aes(week, weekly_case_per100k)) +
theme_minimal() +
geom_line(color = "#00AFBB", size = .4) +
labs(title = "NY Weekly New Cases") 
```

```{r}
ggplot(data=covid_wk_rate_WA, aes(week, weekly_case_per100k)) +
theme_minimal() +
geom_line(color = "#FC4E07", size = .4) +
labs(title = "WA Weekly New Cases") 
```

```{r}
ggplot(data=covid_wk_rate_FL, aes(week, weekly_case_per100k)) +
theme_minimal() +
geom_line(color = "#00AFBB", size = .4) +
labs(title = "FL Weekly New Cases") 
```

iv) (Optional) Use `covid_intervention` to see whether the effectiveness of lockdown in flattening the curve.

## COVID death trend

i) For each month in 2020, plot the monthly deaths per 100k heatmap by state on US map. Use the same color range across months. (Hints: Set `limits` argument in `scale_fill_gradient()` or use `facet_wrap()`; use `lubridate::month()` and `lubridate::year()` to extract month and year from date; use `tidyr::complete(state, month, fill = list(new_case_per100k = NA))` to complete the missing months with no cases.)

```{r}
# create a month index
month <- substr(unique(covid_rate$date), 1, 7)
unique_months <- unique(month)

month_index <- arrange_all(data.frame(unique_months))
index_len <- nrow(month_index)
month_index <- month_index %>% mutate(month = c(1:index_len))
```

```{r}
# create new column to serve as basis of join
covid_rate <- covid_rate %>% mutate (unique_months = (substr(date, 1, 7)))
```

```{r}
# merge the month index
covid_rate <- merge(x=covid_rate,y=month_index,by="unique_months",all.x=TRUE)
```

```{r}
covid_rate_month <- covid_rate %>% group_by(State, month) %>% summarize(weekly_new_cases = sum(new_cases)) 
```

```{r}
# compute "new deaths" as we did "new cases" previously
num_rows <- nrow(covid_rate)
cum_deaths <- covid_rate$cum_deaths
FIPS <- covid_rate$FIPS
new_deaths <-c(rep(NA,num_rows))

for(i in 2:num_rows){
  new_death <- cum_deaths[i] - cum_deaths[i-1]
  if (FIPS[i] == FIPS[i-1]) {
    new_deaths[i]<- new_death
  } else {
    new_deaths[i]<- cum_deaths[i]
  }
}

covid_rate$new_deaths <- new_deaths
```

```{r}
# compute monthly state deaths
covid_state_deaths <- covid_rate %>% group_by(State, month) %>% summarize(monthly_new_deaths = sum(new_deaths)) 
```

```{r}
# merge the state populations column
covid_state_deaths <- merge(x= covid_state_deaths, y= state_pop, on="State", all.x=TRUE)
```

```{r}
# compute deaths per 100k
covid_state_deaths <- covid_state_deaths %>% mutate(monthly_deaths_per100k = (monthly_new_deaths/(state_pop) * 100000))
```

```{r}
# create df for our heatmap

# months 1-12 represent Jan 2020- Dec 2020
heatmap_df <- covid_state_deaths %>% select(State, month, monthly_deaths_per100k) %>% filter(month <= 12)
```

```{r}
# heatmap
ggplot(heatmap_df, aes(month, State, fill= monthly_deaths_per100k)) + 
  geom_tile() +
  ggtitle("Heatmap of monthly deaths per 100k population by state")+
  xlab("Month (Jan 2020 - Dec 2020)")+
  scale_fill_gradientn(colours = c("white", "lightblue", "navy"), na.value = "white") +
  scale_x_continuous(breaks=1:12)
```
**Remarks: Month 1 to 12 represents January to December 2020**

ii) (Optional) Use `plotly` to animate the monthly maps in i). Does it reveal any systematic way to capture the dynamic changes among states? (Hints: Follow *Appendix 3: Plotly heatmap::* in Module 6 regularization lecture to plot the heatmap using `plotly`. Use `frame` argument in `add_trace()` for animation. `plotly` only recognizes abbreviation of state names. Use `unique(us_map(regions = "states") %>% select(abbr, full))` to get the abbreviation and merge with the data to get state abbreviation.)

# COVID factor

We now try to build a good parsimonious model to find possible factors related to death rate on county level. Let us not take time series into account for the moment and use the total number as of *Feb 1, 2021*.

i) Create the response variable `total_death_per100k` as the total of number of COVID deaths per 100k by *Feb 1, 2021*. We suggest to take log transformation as `log_total_death_per100k = log(total_death_per100k + 1)`. Merge `total_death_per100k` to `county_data` for the following analysis.

```{r}
feb1 <- covid_rate %>% filter(date == "2021-02-01")
feb1 <- feb1 %>% mutate(total_death_per100k = (cum_deaths/TotalPopEst2019)*100000)
feb1 <- feb1 %>% mutate(log_total_death_per100k = log(total_death_per100k + 1))
feb1 <- feb1 %>% select(FIPS, total_death_per100k, log_total_death_per100k)
```

```{r}
# merge
county_data <- merge(x=county_data, y=feb1, on="FIPS", all.x=TRUE)
```

ii) Select possible variables in `county_data` as covariates. We provide `county_data_sub`, a subset variables from `county_data`, for you to get started. Please add any potential variables as you wish.

```{r}
# added MedHHInc, PerCapitaInc, LandAreaSQMiles2010, AvgHHSize and PopDensity2010 as possible covariates
# added response variable total_death_per100k at the end

sel_vars <- county_data %>%
  select(County, State, FIPS, MedHHInc, PerCapitaInc, LandAreaSQMiles2010, AvgHHSize, Deep_Pov_All, PovertyAllAgesPct, PerCapitaInc, UnempRate2019, PctEmpFIRE, PctEmpConstruction, PctEmpTrans, PctEmpMining, PctEmpTrade, PctEmpInformation, PctEmpAgriculture, PctEmpManufacturing, PctEmpServices, PopDensity2010, OwnHomePct, Age65AndOlderPct2010, TotalPop25Plus, Under18Pct2010, Ed2HSDiplomaOnlyPct, Ed3SomeCollegePct, Ed4AssocDegreePct, Ed5CollegePlusPct, ForeignBornPct, Net_International_Migration_Rate_2010_2019, NetMigrationRate1019, NaturalChangeRate1019, TotalPopEst2019, WhiteNonHispanicPct2010, NativeAmericanNonHispanicPct2010, BlackNonHispanicPct2010, AsianNonHispanicPct2010, HispanicPct2010, Type_2015_Update, RuralUrbanContinuumCode2013, UrbanInfluenceCode2013, Perpov_1980_0711, HiCreativeClass2000, HiAmenity, Retirement_Destination_2015_Update, total_death_per100k)
```

    a) Report missing values in your final subset of variables. 
    
**ANSWER: ** There were 174 counties with NAs as the response variable in total. 

Upon further investigation (code is below), we found that all counties in the states Alaska, Hawaii, and Puerto Rico have NAs for number of deaths. We will exclude Alaska, Hawaii, and Puerto Rico, totaling 80 counties, from our analysis.

To sum up, we will exclude the data of the following counties from our analysis:

- 120 counties are in states or territories with no data for number of deaths (80 from PR, 34 from AK, 6 and from HI)
- The data for the US as a whole (It is currently included in sel_vars as a county.)
- 53 other counties (We did not see a particular pattern in this group.)


*Reference: code chunk used to check and remove missing data*
```{r}
#check rows with missing data
rownames(sel_vars)[apply(sel_vars, 1, function(x) any(is.na(x)))] 
```

```{r, eval=FALSE}
#view the states with missing data
sel_vars$State[apply(sel_vars, 1, function(x) any(is.na(x)))] 
sel_vars$County[apply(sel_vars, 1, function(x) any(is.na(x)))] 

pr <- sel_vars[State=='PR']
dim(pr)
ak <- sel_vars[State=='AK'] 
dim(ak)
hi <- sel_vars[State=='HI']
dim(hi)
```

```{r}
# dropping rows with missing values
sel_vars <- na.omit(sel_vars)
```
 
    b) In the following anaylsis, you may ignore the missing values.

iii) Use LASSO to choose a parsimonious model with all available sensible county-level information. **Force in State** in the process. Why we need to force in State? You may use `lambda.1se` to choose a smaller model. 

**ANSWER: ** We need to force in state to separate the effects of other state-specific variables not included in the model from the variables that we have selected. This will help in interpreting the coefficients of variables as we analyze the effects of different variables on COVID-related death rates at state level.

However, after forcing 'State' into LASSO, we found that LASSO only returned the 'State' variable.
Therefore, we have chosen to instead use LASSO without the 'State' variable, and then later build a model from variables selected by LASSO and 'State' variables instead.

These are the steps we took in using LASSO based on the information above.

1. Prepare the input X matrix and response Y
```{r}
#Adjust sel_vars to include log_total_death_per100k
sel_vars <- sel_vars %>%
  select(-total_death_per100k) %>%
  left_join(feb1, by="FIPS") %>%
  select(-total_death_per100k)

#Prepare X and Y
Y <- sel_vars$log_total_death_per100k #extract Y
X <- model.matrix(log_total_death_per100k~., data=sel_vars)[, -1] #get X variables as a matrix
```

2. Run LASSO without forcing in 'State'
```{r}
set.seed(10) #to control the randomness in K folds
fit.cv <- cv.glmnet(X, Y, alpha=1, nfolds=10)
```

<!-- 2a. Create index needed to force in 'State' (set the penalty of all State variables as 0, and others as 1) -->

<!-- ```{r} -->
<!-- # create index needed to force in state vars -->
<!-- colx <- colnames(X) -->
<!-- state_index <- c(rep(1, length(colx))) -->

<!-- # we mark State variables with 0s and other variables with 1s -->
<!-- for (i in 1:length(colx)) { -->
<!--   if (startsWith(colx[i], "State")) { -->
<!--     state_index[i] <- 0 -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- 2.5a. Force in 'State' -->
<!-- ```{r} -->
<!-- set.seed(10) #to control the randomness in K folds -->
<!-- fit.force.cv <- cv.glmnet(X, Y, alpha=1, nfolds=10, intercept = T, -->
<!--                        penalty.factor = state_index) -->
<!-- ``` -->

3. Output $\beta$'s from lambda.1se
```{r}
#extract non-zero coefficients and the predictors
coef.1se <- coef(fit.cv, s="lambda.1se")
coef.1se <- coef.1se[which(coef.1se !=0),] #extract non-zero coefficients
coef.1se
var.1se <- rownames(as.matrix(coef.1se))[-1] #save the names of coef
```

iv) Use `Cp` or BIC to fine tune the LASSO model from iii). Again **force in State** in the process. 
**ANSWER:** These are the steps we took:

1. Create a new data frame sel_vars.sub that stores all the variables we will use

Note that we will also add "Age65AndOlderPct2010" as one of the variables to consider, because it has been shown that COVID affects death rates in older people more.
```{r}
#create index to pull selected variables from sel_vars
##create dummy variables for County and States and store them in sel_vars.new
sel_vars$Count <- as.factor(sel_vars$County)
sel_vars$State <- as.factor(sel_vars$State)
sel_vars.new <- one_hot(as.data.table(sel_vars))

##rename column names in sel_vars.new to match var.1se
colnames(sel_vars.new) <-gsub("Count_", "County", colnames(sel_vars.new)) 
colnames(sel_vars.new) <-gsub("State_", "State", colnames(sel_vars.new)) 

#Create sel_vars.sub to store variables to be used in our model
sel_vars.sub <- sel_vars.new %>%
  select(all_of(var.1se), starts_with("State"), "log_total_death_per100k", "Age65AndOlderPct2010") %>%
  select(-StateAL) #drop AL to make it base case

names(sel_vars.sub)
```

2. Reorder the columns in sel_vars.sub to prepare for using regsubsets
```{r}
# let's do some column reordering which will help using force.in later while using regsubsets

coln <- colnames(sel_vars.sub)
state_vars <- c()
nonstate_vars <- c()
for (i in 1:length(coln)) {
  if (startsWith(coln[i], "State")) {
    state_vars <- c(state_vars, coln[i])
  } else {
    nonstate_vars <- c(nonstate_vars, coln[i])
  }
}

reordered_cols <- c(state_vars, nonstate_vars)
sel_vars.sub <- sel_vars.sub %>% select(all_of(reordered_cols))
```

3. Use regsubsets to select a subset with Cp (note: force all State categorical variables in)

We first use regsubset. We will use method = "forward" because we have 97 variables 

```{r}
fit.1 <- regsubsets(log_total_death_per100k~., nvmax = 97, method = "forward", force.in = c(1:48), really.big=TRUE, sel_vars.sub)
    
summary(fit.1)
```

Next, we look at the plots of Cp values
```{r}
plot(summary(fit.1)$cp, xlab="Number of predictors", ylab="cp", col="red", pch=16)  
```
While we can see that the drop in Cp becomes smaller as the number of predictors increases,
the difference in the drop in Cp when there are only a few variables are not a lot larger than others.

From the plot, we will apply the label rule and select 12 variables
```{r, include=FALSE}
# opt.size <- which.min(summary(fit.1)$cp)
```

```{r}
opt.size <- 12
```

Now we will look for the optimal variables selected.
```{r}
fit.1.var <- summary(fit.1)$which
fit.1.var[opt.size,]

```

Pull out the variable names:
```{r}
colnames(fit.1.var)[fit.1.var[opt.size,]]
final_colnames <- colnames(fit.1.var)[fit.1.var[opt.size,]][-1]
```

Next, we fit these variables into a linear regression.
```{r}
#create sel_vars.final with only variables in the final model
sel_vars.final <- sel_vars.sub %>%
  select(log_total_death_per100k, all_of(final_colnames))

#create our final model
final.fit <- lm(log_total_death_per100k~., sel_vars.final)
summary(final.fit)
```

We then check the residual plot and qqplot of the residuals of the model above to check whether the model assumptions are met.
```{r, warning=FALSE}
par(mfrow=c(1,2), mar=c(5,2,4,2), mgp=c(3,0.5,0)) # plot(fit3) produces several plots
plot(final.fit, 1, pch=16) # residual plot
abline(h=0, col="blue", lwd=2)
plot(final.fit, 2) # qqplot
```

We can see from the two plots above that there is a strange pattern in the residual plot, which is the downward sloping line in the bottom part of the plot. Moreover, the normality assumption is likely violated since the left tail end of the normal quantile plot strays very far from the 45-degree line. Upon further investigation, we found that the anomalies in the two plots are the same counties, and that the number of deaths in these counties are all zero.

Because of this, we will remove the counties with zero COVID deaths and to recreate the model with the same variables.

```{r}
#remove counties with zero deaths from the dataset (64 counties in total)
sel_vars.final.fixed <- sel_vars.final %>%
  filter(log_total_death_per100k > 1)

#recreate our final model
final.fit.fixed <- lm(log_total_death_per100k~., sel_vars.final.fixed)
summary(final.fit.fixed)
```

We will take a quick comparison between the two models (final.fit and final.fit.fixed).
```{r, results='asis'}
stargazer(final.fit, final.fit.fixed, type = output_format, 
                     # ci=TRUE, ci.level = .95,
                     keep.stat = c("n", "rsq", "sigma2", "ser"))
```
We can see that there were no significant changes in the coefficients of the continuous variables in both models.
This means that, we can also use final.fit to analyze the effects of different factors on log_total_death_per100k death rate.

We will use final.fit.fixed as our final model.

v) If necessary, reduce the model from iv) to a final model with all variables being significant at 0.05 level. Are the linear model assumptions all reasonably met?

**Answer:** 
Yes, the assumptions are reasonably met. Although it should be noted that the distribution of errors are a little left-skewed.
Please refer to the residual plot and the qqplot of the residuals below.

```{r, warning=FALSE}
par(mfrow=c(1,2), mar=c(5,2,4,2), mgp=c(3,0.5,0)) # plot(fit3) produces several plots
plot(final.fit.fixed, 1, pch=16) # residual plot
abline(h=0, col="blue", lwd=2)
plot(final.fit.fixed, 2) # qqplot
```

vi) It has been shown that COVID affects elderly the most. It is also claimed that the COVID death rate among African Americans and Latinos is higher. Does your analysis support these arguments?

**ANSWER: ** Yes, the analysis supports both arguments. 

*Regarding how COVID affects the elderly*
Age65AndOlderPct2010 has a coefficent of 0.03, which indicates that on average, if the percentage of people above 65 years old increases by 1, the number of total COVID-related deaths per 100k population is expected to increase by 3%.

*Regarding how COVID affects the African Americans*
WhiteNonHispanicPct2010 has a coefficient of -0.007, which indicates that on average, if the percentage of white population increases by 1, the number of total COVID-related deaths per 100k population is expected to decrease by 0.7%.
Since African Americans and Latinos make up approximately 75% of non-whites across the united states, we can assume that COVID death rate among them are indeed higher.

vii) Based on your final model, summarize your findings. In particular, summarize the state effect controlling for others. Provide intervention recommendations to policy makers to reduce COVID death rate.

**ANSWER: ** 
We can see that the state effect are significant when controlling for others. (Please refer to portion "Test whether State variables are zero" below for how we tested for the state effect's significance.) We can see from the heatmap plot of the state categorical variables on US map (below), that when controlling for other variables, the states in the south, along with some on the eastern coast, have a higher number of covid deaths per 100k population on average.

Also, as stated previously, we find based on our final model that counties with high percentage of elderly population have higher death rates on average. 

We recommend that aid be directed specifically to hospitals in counties with higher percentage of elderly population, to increase the capacity to treat the elderly population. Also more resources can be directed to these counties in states that have higher coefficients, which tend to be sttes in the south. This is so that those who are most vulnerable to covid19 can immediately receive treatment.

**Test whether State variables are zero**
<br>
The test below shows that state variables are significant controlling for other variables selected in the final model.
```{r}
# We first fit a linear model regression without the state variables
data.nonstate <- sel_vars.final.fixed %>% select(-all_of(state_vars))
nonstate.fit <- lm(log_total_death_per100k~., data.nonstate)

# Then we use little anova to test whether State variables are zero
anova(nonstate.fit, final.fit.fixed)
```

**Heatmap plot**
```{r}
#prepare data for the map
##Put data on all coefficients into a dataframe
data.coef.map <- as.data.frame(summary(final.fit.fixed)$coef)
data.coef.map$state <- rownames(data.coef.map) #create a new column for variable names
rownames(data.coef.map) <- NULL

##Filter only state variables
data.coef.map <- data.coef.map %>%
  filter(grepl('State', state))

##Remove string 'State' from all variables
data.coef.map$state <- gsub("State", "", data.coef.map$state)

# set color range limits
max_col <- quantile(data.coef.map$estimate, .98, na.rm = T)
min_col <- quantile(data.coef.map$estimate, 0, na.rm = T)

#plot
coef.map.plot <- plot_usmap(regions = "state",                   #regions = "counties", for county level summary
    data = data.coef.map,
    values = "Estimate", exclude = c("Hawaii", "Alaska", "Puerto Rico "), color = "black") +
    scale_fill_distiller(palette = "GnBu", 
                         direction = 1,
                         # limits sets range of color
                         name = "Coefficient of state categorical variables",
                         limits = c(min_col, max_col)) +

    labs(title = "Heatmap: coefficient of state categorical variables", subtitle = "excluding AK, HI, PR") +
    theme(legend.position = "right")

ggplotly(coef.map.plot +
           theme(legend.position = "none"))
```


viii) What else can we do to improve our model? What other important information we may have missed? 

**ANSWER: ** One potentially predictive variable is the number of hospitals/health care practitioners per county. We can go further by using number of hospital beds/hospitals/doctors per 100k citizens of the county. We hypothesize that these variables are inversely correlated with death rates. Having information on how much testing, which likely leads to early detection, and immediate treatment is available in each state compared to the size of population may significantly improve the model as well. 

ix) (Optional) Would your findings be very different if you had refined the data in some way or imputed the missing values in part ii). Check PCA lecture, section 10 for imputations via `softImpute`. 


# Appendix 1: Data description {-}

A detailed summary of the variables in each data set follows:

**Infection and fatality data**

* date: Date
* county: County name
* state: State name
* fips: County code that uniquely identifies a county
* cases: Number of cumulative COVID-19 infections
* deaths: Number of cumulative COVID-19 deaths

**Socioeconomic demographics**

*Income*: Poverty level and household income 

* PovertyUnder18Pct: Poverty rate for children age 0-17, 2018  
* Deep_Pov_All: Deep poverty, 2014-18  
* Deep_Pov_Children: Deep poverty for children, 2014-18  
* PovertyAllAgesPct: Poverty rate, 2018  
* MedHHInc: Median household income, 2018 (In 2018 dollars)  
* PerCapitaInc: Per capita income in the past 12 months (In 2018 inflation adjusted dollars), 2014-18  
* PovertyAllAgesNum: Number of people of all ages in poverty, 2018  
* PovertyUnder18Num: Number of people age 0-17 in poverty, 2018   

*Jobs*: Employment type, rate, and change 

* UnempRate2007-2019: Unemployment rate, 2007-2019 
* NumEmployed2007-2019: Employed, 2007-2019
* NumUnemployed2007-2019: Unemployed, 2007-2019

* PctEmpChange1019: Percent employment change, 2010-19  
* PctEmpChange1819: Percent employment change, 2018-19  
* PctEmpChange0719: Percent employment change, 2007-19  
* PctEmpChange0710: Percent employment change, 2007-10  

* NumCivEmployed: Civilian employed population 16 years and over, 2014-18  
* NumCivLaborforce2007-2019: Civilian labor force, 2007-2019  


* PctEmpFIRE: Percent of the civilian labor force 16 and over  employed in finance and insurance, and real estate and rental and leasing, 2014-18    
* PctEmpConstruction: Percent of the civilian labor force 16 and over employed in construction, 2014-18  
* PctEmpTrans: Percent of the civilian labor force 16 and over employed in transportation, warehousing and utilities, 2014-18 
* PctEmpMining: Percent of the civilian labor force 16 and over employed in mining, quarrying, oil and gas extraction, 2014-18  
* PctEmpTrade: Percent of the civilian labor force 16 and over employed in wholesale and retail trade, 2014-18  
* PctEmpInformation: Percent of the civilian labor force 16 and over employed in information services, 2014-18   
* PctEmpAgriculture: Percent of the civilian labor force 16 and over employed in agriculture, forestry, fishing, and hunting, 2014-18   
* PctEmpManufacturing: Percent of the civilian labor force 16 and over employed in manufacturing, 2014-18    
* PctEmpServices: Percent of the civilian labor force 16 and over employed in services, 2014-18    
* PctEmpGovt: Percent of the civilian labor force 16 and over employed in public administration, 2014-18 

*People*: Population size, density, education level, race, age, household size, and migration rates

* PopDensity2010: Population density, 2010  
* LandAreaSQMiles2010: Land area in square miles, 2010 

* TotalHH: Total number of households, 2014-18  
* TotalOccHU: Total number of occupied housing units, 2014-18  
* AvgHHSize: Average household size, 2014-18   
* OwnHomeNum: Number of owner occupied housing units, 2014-18  
* OwnHomePct: Percent of owner occupied housing units, 2014-18
* NonEnglishHHPct: Percent of non-English speaking households of total households, 2014-18     
* HH65PlusAlonePct: Percent of persons 65 or older living alone, 2014-18  
* FemaleHHPct: Percent of female headed family households of total households, 2014-18  
* FemaleHHNum: Number of female headed family households, 2014-18  
* NonEnglishHHNum: Number of non-English speaking households, 2014-18  
* HH65PlusAloneNum: Number of persons 65 years or older living alone, 2014-18

* Age65AndOlderPct2010: Percent of population 65 or older, 2010
* Age65AndOlderNum2010: Population 65 years or older, 2010  
* TotalPop25Plus: Total population 25 and older, 2014-18 - 5-year average  
* Under18Pct2010: Percent of population under age 18, 2010  
* Under18Num2010: Population under age 18, 2010

*  Ed1LessThanHSPct: Percent of persons with no high school diploma or GED, adults 25 and over, 2014-18  
* Ed2HSDiplomaOnlyPct: Percent of persons with a high school diploma or GED only, adults 25 and over, 2014-18    
* Ed3SomeCollegePct: Percent of persons with some college experience, adults 25 and over, 2014-18    
* Ed4AssocDegreePct: Percent of persons with an associate's degree, adults 25 and over, 2014-18    
* Ed5CollegePlusPct: Percent of persons with a 4-year college degree or more, adults 25 and over, 2014-18    
* Ed1LessThanHSNum: No high school, adults 25 and over, 2014-18
* Ed2HSDiplomaOnlyNum: High school only, adults 25 and over, 2014-18  
* Ed3SomeCollegeNum: Some college experience, adults 25 and over, 2014-18   
* Ed4AssocDegreeNum: Number of persons with an associate's degree, adults 25 and over, 2014-18   
* Ed5CollegePlusNum: College degree 4-years or more, adults 25 and over, 2014-18   

* ForeignBornPct: Percent of total population foreign born, 2014-18  
* ForeignBornEuropePct: Percent of persons born in Europe, 2014-18  
* ForeignBornMexPct: Percent of persons born in Mexico, 2014-18
* ForeignBornCentralSouthAmPct: Percent of persons born in Central or South America, 2014-18  
* ForeignBornAsiaPct: Percent of persons born in Asia, 2014-18
* ForeignBornCaribPct: Percent of persons born in the Caribbean, 2014-18  
* ForeignBornAfricaPct: Percent of persons born in Africa, 2014-18   
* ForeignBornNum: Number of people foreign born, 2014-18  
* ForeignBornCentralSouthAmNum: Number of persons born in Central or South America, 2014-18   
* ForeignBornEuropeNum: Number of persons born in Europe, 2014-18  
* ForeignBornMexNum: Number of persons born in Mexico, 2014-18
* ForeignBornAfricaNum: Number of persons born in Africa, 2014-18  
* ForeignBornAsiaNum: Number of persons born in Asia, 2014-18 
* ForeignBornCaribNum: Number of persons born in the Caribbean, 2014-18   

* Net_International_Migration_Rate_2010_2019: Net international migration rate, 2010-19  
* Net_International_Migration_2010_2019: Net international migration, 2010-19   
* Net_International_Migration_2000_2010: Net international migration, 2000-10  
* Immigration_Rate_2000_2010: Net international migration rate, 2000-10   
* NetMigrationRate0010: Net migration rate, 2000-10   
* NetMigrationRate1019: Net migration rate, 2010-19  
* NetMigrationNum0010: Net migration, 2000-10  
* NetMigration1019: Net Migration, 2010-19  

* NaturalChangeRate1019: Natural population change rate, 2010-19   
* NaturalChangeRate0010: Natural population change rate, 2000-10    
* NaturalChangeNum0010: Natural change, 2000-10  
* NaturalChange1019: Natural population change, 2010-19

* TotalPop2010: Population size 4/1/2010 Census 
* TotalPopEst2010: Population size 7/1/2010
* TotalPopEst2011: Population size 7/1/2011
* TotalPopEst2012: Population size 7/1/2012
* TotalPopEst2013: Population size 7/1/2013
* TotalPopEst2014: Population size 7/1/2014
* TotalPopEst2015: Population size 7/1/2015
* TotalPopEst2016: Population size 7/1/2016
* TotalPopEst2017: Population size 7/1/2017
* TotalPopEst2018: Population size 7/1/2018
* TotalPopEst2019: Population size 7/1/2019
* TotalPopACS: Total population, 2014-18 - 5-year average   
* TotalPopEstBase2010: County Population estimate base 4/1/2010

* NonHispanicAsianPopChangeRate0010: Population change rate Non-Hispanic Asian, 2000-10  
* PopChangeRate1819: Population change rate, 2018-19    
* PopChangeRate1019: Population change rate, 2010-19    
* PopChangeRate0010: Population change rate, 2000-10   
* NonHispanicNativeAmericanPopChangeRate0010: Population change rate Non-Hispanic Native American, 2000-10    
* HispanicPopChangeRate0010: Population change rate Hispanic, 2000-10  
* MultipleRacePopChangeRate0010: Population change rate multiple race, 2000-10    
* NonHispanicWhitePopChangeRate0010: Population change rate Non-Hispanic White, 2000-10  
* NonHispanicBlackPopChangeRate0010: Population change rate Non-Hispanic African American, 2000-10  

* MultipleRacePct2010: Percent multiple race, 2010  
* WhiteNonHispanicPct2010: Percent Non-Hispanic White, 2010    
* NativeAmericanNonHispanicPct2010: Percent Non-Hispanic Native American, 2010  
* BlackNonHispanicPct2010: Percent Non-Hispanic African American, 2010    
* AsianNonHispanicPct2010: Percent Non-Hispanic Asian, 2010   
* HispanicPct2010: Percent Hispanic, 2010  
* MultipleRaceNum2010: Population size multiple race, 2010   
* WhiteNonHispanicNum2010: Population size Non-Hispanic White, 2010    
* BlackNonHispanicNum2010: Population size Non-Hispanic African American, 2010  
* NativeAmericanNonHispanicNum2010: Population size Non-Hispanic Native American, 2010   
* AsianNonHispanicNum2010: Population size Non-Hispanic Asian, 2010    
* HispanicNum2010: Population size Hispanic, 2010

*County classifications*: Type of county (rural or urban on a rural-urban continuum scale)

* Type_2015_Recreation_NO: Recreation counties, 2015 edition  
* Type_2015_Farming_NO: Farming-dependent counties, 2015 edition  
* Type_2015_Mining_NO: Mining-dependent counties, 2015 edition
* Type_2015_Government_NO: Federal/State government-dependent counties, 2015 edition  
* Type_2015_Update: County typology economic types, 2015 edition   
* Type_2015_Manufacturing_NO: Manufacturing-dependent counties, 2015 edition  
* Type_2015_Nonspecialized_NO: Nonspecialized counties, 2015 edition    
* RecreationDependent2000: Nonmetro recreation-dependent, 1997-00  
* ManufacturingDependent2000: Manufacturing-dependent, 1998-00
* FarmDependent2003: Farm-dependent, 1998-00  
* EconomicDependence2000: Economic dependence, 1998-00  

* RuralUrbanContinuumCode2003: Rural-urban continuum code, 2003
* UrbanInfluenceCode2003: Urban influence code, 2003  
* RuralUrbanContinuumCode2013: Rural-urban continuum code, 2013
* UrbanInfluenceCode2013: Urban influence code, 2013  
* Noncore2013: Nonmetro noncore, outside Micropolitan and Metropolitan, 2013  
* Micropolitan2013: Micropolitan, 2013
* Nonmetro2013: Nonmetro, 2013
* Metro2013: Metro, 2013  
* Metro_Adjacent2013: Nonmetro, adjacent to metro area, 2013  
* Noncore2003: Nonmetro noncore, outside Micropolitan and Metropolitan, 2003  
* Micropolitan2003: Micropolitan, 2003  
* Metro2003: Metro, 2003  
* Nonmetro2003: Nonmetro, 2003  
* NonmetroNotAdj2003: Nonmetro, nonadjacent to metro area, 2003
* NonmetroAdj2003: Nonmetro, adjacent to metro area, 2003  

* Oil_Gas_Change: Change in the value of onshore oil and natural gas production, 2000-11  
* Gas_Change: Change in the value of onshore natural gas production, 2000-11    
* Oil_Change: Change in the value of onshore oil production, 2000-11  

* Hipov: High poverty counties, 2014-18  
* Perpov_1980_0711: Persistent poverty counties, 2015 edition  
* PersistentChildPoverty_1980_2011: Persistent child poverty counties, 2015 edition  
* PersistentChildPoverty2004: Persistent child poverty counties, 2004  
* PersistentPoverty2000: Persistent poverty counties, 2004

* Low_Education_2015_update: Low education counties, 2015 edition
* LowEducation2000: Low education, 2000  

* HiCreativeClass2000: Creative class, 2000  
* HiAmenity: High natural amenities  
* RetirementDestination2000: Retirement destination, 1990-00  
* Low_Employment_2015_update: Low employment counties, 2015 edition
* Population_loss_2015_update: Population loss counties, 2015 edition
* Retirement_Destination_2015_Update: Retirement destination counties, 2015 edition

# Appendix 2: Data cleaning {-}

The raw data sets are dirty and need transforming before we can do our EDA.
It takes time and efforts to clean and merge different data sources so we provide the final output of the cleaned and merged data. The cleaning procedure is as follows. Please read through to understand what is in the cleaned data. We set `eval = data_cleaned` in the following cleaning chunks so that these cleaning chunks will only run if any of `data/covid_county.csv`, `data/covid_rates.csv` or `data/covid_intervention.csv`  does not exist.

```{r}
# Indicator to check whether the data files exist
data_cleaned <- !(file.exists("data/covid_county.csv") & 
                    file.exists("data/covid_rates.csv") & 
                    file.exists("data/covid_intervention.csv"))
```

We first read in the table using `data.table::fread()`, as we did last time.

```{r Read in covid data, eval = data_cleaned}
# COVID case/mortality rate data
covid_rates <- fread("data/us_counties.csv", na.strings = c("NA", "", "."))
nyc <- fread("data/nycdata.csv", na.strings = c("NA", "", "."))

# Socioeconomic data
income <- fread("data/income.csv", na.strings = c("NA", "", "."))
jobs <- fread("data/jobs.csv", na.strings = c("NA", "", "."))
people <- fread("data/people.csv", na.strings = c("NA", "", "."))
county_class <- fread("data/county_classifications.csv", na.strings = c("NA", "", "."))

# Internvention policy data
int_dates <- fread("data/intervention_dates.csv", na.strings = c("NA", "", "."))
```

## Clean NYC data

The original NYC data contains more information than we need. We extract only the number of cases and deaths and format it the same as the `covid_rates` data.

```{r Clean NYC data, eval = data_cleaned}
# NYC county fips matching table 
nyc_fips <- data.table(FIPS = c('36005', '36047', '36061', '36081', '36085'),
                       County = c("BX", "BK", "MN", "QN", "SI"))

# nyc case
nyc_case <- nyc[,.(date = as.Date(date_of_interest, "%m/%d/%Y"),
                   BX = BX_CASE_COUNT, 
                   BK = BK_CASE_COUNT, 
                   MN = MN_CASE_COUNT, 
                   QN = QN_CASE_COUNT, 
                   SI = SI_CASE_COUNT)]

nyc_case %<>% 
  pivot_longer(cols = BX:SI, 
               names_to = "County",
               values_to = "cases") %>%
  arrange(date) %>%
  group_by(County) %>%
  mutate(cum_cases = cumsum(cases))

# nyc death
nyc_death <- nyc[,.(date = as.Date(date_of_interest, "%m/%d/%Y"),
                   BX = BX_DEATH_COUNT, 
                   BK = BK_DEATH_COUNT, 
                   MN = MN_DEATH_COUNT, 
                   QN = QN_DEATH_COUNT, 
                   SI = SI_DEATH_COUNT)]

nyc_death %<>% 
  pivot_longer(cols = BX:SI, 
               names_to = "County",
               values_to = "deaths") %>%
  arrange(date) %>%
  group_by(County) %>%
  mutate(cum_deaths = cumsum(deaths))

nyc_rates <- merge(nyc_case, 
                   nyc_death,
                   by = c("date", "County"),
                   all.x= T)

nyc_rates <- merge(nyc_rates,
                   nyc_fips,
                   by = "County")

nyc_rates$State <- "New York"
nyc_rates %<>% 
  select(date, FIPS, County, State, cum_cases, cum_deaths) %>%
  arrange(FIPS, date)
```
  
## Continental US cases

We only consider cases and death in continental US. Alaska, Hawaii, and Puerto Rico have 02, 15, and 72 as their respective first 2 digits of their FIPS. We use the `%/%` operator for integer division to get the first 2 digits of FIPS. We also remove Virgin Islands and Northern Mariana Islands. All data of counties in NYC are aggregated as `County == "New York City"` in `covid_rates` with no FIPS, so we combine the NYC data into `covid_rate`.

```{r, eval = data_cleaned}
covid_rates <- covid_rates %>% 
  arrange(fips, date) %>%
  filter(!(fips %/% 1000 %in% c(2, 15, 72))) %>%
  filter(county != "New York City") %>%
  filter(!(state %in% c("Virgin Islands", "Northern Mariana Islands"))) %>%
  rename(FIPS = "fips",
         County = "county",
         State = "state",
         cum_cases = "cases", 
         cum_deaths = "deaths")


covid_rates$date <- as.Date(covid_rates$date)

covid_rates <- rbind(covid_rates, 
                     nyc_rates)
```

## COVID date to week

We set the week of Jan 21, 2020 (the first case of COVID case in US) as the first week (2020-01-19 to 2020-01-25).

```{r, eval = data_cleaned}
covid_rates[, week := (interval("2020-01-19", date) %/% weeks(1)) + 1]
```

## COVID infection/mortality rates

Merge the `TotalPopEst2019` variable from the demographic data with `covid_rates` by FIPS.
  
```{r, eval = data_cleaned}
covid_rates <- merge(covid_rates[!is.na(FIPS)], 
                     people[,.(FIPS = as.character(FIPS),
                                   TotalPopEst2019)],
                     by = "FIPS",  
                     all.x = TRUE)
```


## NA in COVID data

NA values in the `covid_rates` data set correspond to a county not having confirmed cases/deaths. We replace the NA values in these columns with zeros. FIPS for Kansas city, Missouri, Rhode Island and some others are missing. We drop them for the moment and output the data up to week 57 as `covid_rates.csv`.

```{r, eval = data_cleaned}
covid_rates$cum_cases[is.na(covid_rates$cum_cases)] <- 0
covid_rates$cum_deaths[is.na(covid_rates$cum_deaths)] <- 0
```

```{r, eval = data_cleaned}
fwrite(covid_rates %>% 
         filter(week < 58) %>% 
         arrange(FIPS, date), 
       "data/covid_rates.csv")
```

## Formatting date in `int_dates`

We convert the columns representing dates in `int_dates` to R Date types using `as.Date()`. 
We will need to specify that the `origin` parameter is `"0001-01-01"`. 
We output the data as `covid_intervention.csv`.

```{r, eval = data_cleaned}
int_dates <- int_dates[-1,]
date_cols <- names(int_dates)[-(1:3)]
int_dates[, (date_cols) := lapply(.SD, as.Date, origin = "0001-01-01"), 
          .SDcols = date_cols]

fwrite(int_dates, "data/covid_intervention.csv")
```

## Merge demographic data

Merge the demographic data sets by FIPS and output as `covid_county.csv`.

```{r, eval = data_cleaned}
countydata <-
  merge(x = income,
        y = merge(
          x = people,
          y = jobs,
          by = c("FIPS", "State", "County")),
        by = c("FIPS", "State", "County"),
        all = TRUE)


countydata <- 
  merge(
    x = countydata,
    y = county_class %>% rename(FIPS = FIPStxt), 
    by = c("FIPS", "State", "County"),
    all = TRUE
  )

# Check dimensions
# They are now 3279 x 208
dim(countydata)
fwrite(countydata, "data/covid_county.csv")
```

